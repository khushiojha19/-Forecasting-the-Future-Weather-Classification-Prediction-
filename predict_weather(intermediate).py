# -*- coding: utf-8 -*-
"""Predict_Weather(Intermediate).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b8IT4E0ldBNuKjKbb9dQvHC4KCNpFZMj

**Importing Libraries**
"""

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

"""**Load the Dataset**"""

x = 'weather_data.csv'

df = pd.read_csv(x)

"""**Inspect the Dataset**"""

# Display the first 5 rows of the Dataset

print(df.head())

# Find the general information of each column

print(df.info())

"""**Feature Engineering**

**1. Create a new column called "Feels Like Temperature"**
"""

# FeelsLike=Temperature−(WindSpeed×0.2)

df['Feels Like Temperature'] = df['temp'] - (df['wind_spd'] * 2)

print(df[['temp', 'wind_spd', 'Feels Like Temperature']].head())

"""**2. Detect and handle outliers in Temperature, Humidity, and Pressure using the IQR method**"""

# Plot a Box Plot to visualize outliers


# Creating a figure with 3 sub-plots
plt.figure(figsize = (15,5))


# Temperature Boxplot

plt.subplot(1,3,1)

plt.title('Boxplot of Temperature')

sns.boxplot(y=df['temp'])


# Humidity Boxplot

plt.subplot(1,3,2)

plt.title('Boxplot of Humidity')

sns.boxplot(y = df['rh'])


# Pressure Boxplot

plt.subplot(1,3,3)

plt.title('Boxplot of Pressure')

sns.boxplot(y = df['pres'])


plt.show()

# Function to detect and remove outliers using IQR Method

def rem_outlier(df, column):

  Q1 = df[column].quantile(0.25)

  Q3 = df[column].quantile(0.75)

  IQR = Q3 - Q1


  # Define lower and upper bounds

  lower_b = Q1 - 1.5 * IQR
  upper_b = Q3 + 1.5 * IQR


  # Filter the dataframe keeping only the values within bound

  df_filter = df[(df[column] >= lower_b) & (df[column] <= upper_b)]

  return df_filter


# Apply the function to Temperature, Humidity and Pressure

df_clean = rem_outlier(df, 'temp')

df_clean = rem_outlier(df_clean, 'rh')

df_clean = rem_outlier(df_clean, 'pres')


# Display the summary after removing outliers

print(df_clean[['temp', 'rh', 'pres']].head())

"""**3. Encode Weather Condition into numerical values using Label Encoding or One-Hot Encoding**"""

from sklearn.preprocessing import LabelEncoder


# Create a Label Encoder Object

label_e = LabelEncoder()


# Apply Label Encoding on 'Description' column

df['weather_encoded'] = label_e.fit_transform(df['description(output)'])


# Display the first 5 rows
print(df[['description(output)', 'weather_encoded']].head())


# Display the last 5 rows
print(df[['description(output)', 'weather_encoded']].tail())

"""**4. Normalize Temperature, Humidity, and Wind Speed using Min-Max Scaling**"""

from sklearn.preprocessing import MinMaxScaler


# Create a MinMaxScaler Object
norm = MinMaxScaler()


# Select columns to normlaize

col_norm = ['temp', 'rh', 'wind_spd']


# Aplly MinMaxScaking

df[col_norm] = norm.fit_transform(df[col_norm])


# Display first 5 rows

print(df[['temp', 'rh', 'wind_spd']].head())

"""**5. Use groupby() to find average temperature per month**"""

# Convert Date to DateTIme format

df['datetime'] = pd.to_datetime(df['datetime'], format = '%Y-%m-%d:%H')

# Extract month from the Date column

df['month'] = df['datetime'].dt.month


# Find the average temperature per month

avg_temp = df.groupby('month')['temp'].mean()


print(avg_temp)

"""**MACHINE LEARNING**

**Regression Tasks**

**Predict Temperature using features like Humidity, Wind Speed, and Pressure**

**Train models: Linear Regression, Decision Tree, and Random Forest**

**Evaluate models using RMSE and R² score**

**Use feature importance to determine the most significant predictors**
"""

# Importing Libraries

import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.ensemble import RandomForestRegressor

from sklearn.tree import DecisionTreeRegressor

from sklearn.metrics import mean_squared_error, r2_score

# Defining features 'X' and target 'y'

X = df[['rh', 'pres', 'wind_spd']]

y = df['temp']

# Split data into training set and testing set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# Train models


# Linear Regression Model

lr = LinearRegression()

lr.fit(X_train, y_train)


# Decision Tree Regressor Model

dt = DecisionTreeRegressor(random_state = 42)

dt.fit(X_train, y_train)


# Random Forest Regressor Model

rf = RandomForestRegressor(n_estimators = 100, random_state = 42)

rf.fit(X_train, y_train)

# Make predictions


# Predict the Linear Regression Model

y_pred_lr = lr.predict(X_test)


# Predict the Decision Tree Regressor Model

y_pred_dt = dt.predict(X_test)


# Predict the Random Forest Regressor Model

y_pred_rf = rf.predict(X_test)

# Evaluate Models using RMSE and R2 Score

def evaluate_model(model_name, y_true, y_pred):

  rmse = np.sqrt(mean_squared_error(y_true, y_pred))

  r2 = r2_score(y_true, y_pred)


  print(f"{model_name} : RMSE: {rmse} and R2 Score: {r2}")



evaluate_model("Linear Regression", y_test, y_pred_lr)

evaluate_model("Decision Tree", y_test, y_pred_dt)

evaluate_model("Random Forest", y_test, y_pred_rf)

# Feature Importance for Decision Tree & Random Forest Models


# Get feature importance from both models

feat_impt_dt = dt.feature_importances_

feat_impt_rf = rf.feature_importances_


# Convert feature importance into DataFrames for better readability

feat_df_dt = pd.DataFrame({'Feature': X.columns, 'Importance': feat_impt_dt})

feat_df_rf = pd.DataFrame({'Feature': X.columns, 'Importance': feat_impt_rf})


# Sort the DataFrames based on importance in descending order

feat_df_dt = feat_df_dt.sort_values(by='Importance', ascending=False)

feat_df_rf = feat_df_rf.sort_values(by='Importance', ascending=False)


# Print Feature Importance for Decision Tree and Random Forest

print("\nFeature Importance (Decision Tree):")
print(feat_df_dt)

print("\nFeature Importance (Random Forest):")
print(feat_df_rf)

# Plotting Feature Importance for Decision Tree


plt.figure(figsize=(8, 5))

plt.bar(feat_df_dt['Feature'], feat_df_dt['Importance'], color='lightblue')

plt.xlabel("Features")

plt.ylabel("Importance Score")

plt.title("Feature Importance (Decision Tree)")

plt.show()

# Plotting Feature Importance for Random Forest


plt.figure(figsize=(8, 5))

plt.bar(feat_df_rf['Feature'], feat_df_rf['Importance'], color='lightpink')

plt.xlabel("Features")

plt.ylabel("Importance Score")

plt.title("Feature Importance (Random Forest)")

plt.show()

"""**Classification Tasks**

**Predict Weather Condition (Sunny, Cloudy, Rainy, etc.) based on numerical features**

**Train models: Logistic Regression, Random Forest, and SVM**

**Evaluate using Confusion Matrix and F1-score**
"""

from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import f1_score, accuracy_score, confusion_matrix

from sklearn.svm import SVC

from sklearn.linear_model import LogisticRegression

from sklearn.ensemble import RandomForestClassifier

# Defining features 'X' and target 'y'

X = df[['temp', 'rh','wind_spd', 'pres']]

y = df['description(output)']

# Encode the target variable

label_enc = LabelEncoder()

y_encoded = label_enc.fit_transform(y)

# Split the data into training and testing datasets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# Train the Models


# Logistic Regression Model

lr_model = LogisticRegression(max_iter = 1000)

lr_model.fit(X_train, y_train)


# Random Forest Classifier Model

rf_model = RandomForestClassifier(n_estimators = 100, random_state = 42)

rf_model.fit(X_train, y_train)


# Support Vector Machine (SVM) Model

svm_model = SVC(kernel='linear', random_state=42)

svm_model.fit(X_train, y_train)

# Make predictions on the test set

y_pred_lrm = lr_model.predict(X_test)

y_pred_rfc = rf_model.predict(X_test)

y_pred_svm = svm_model.predict(X_test)

# Evaluate the Models

# 1. Confusion Matrix


cm_lrm = confusion_matrix(y_test, y_pred_lrm)

cm_rfc = confusion_matrix(y_test, y_pred_rfc)

cm_svm = confusion_matrix(y_test, y_pred_svm)


# 2. F1 Score


f1_lrm = f1_score(y_test, y_pred_lrm, average = 'weighted')

f1_rfc = f1_score(y_test, y_pred_rfc, average = 'weighted')

f1_svm = f1_score(y_test, y_pred_svm, average = 'weighted')


# 3. Accuracy Score


acc_lrm = accuracy_score(y_test, y_pred_lrm)

acc_rfc = accuracy_score(y_test, y_pred_rfc)

acc_svm = accuracy_score(y_test, y_pred_svm)

# Print Evaluation Metrics for all models

print(f"\n Logistic Regression : Accuracy : {acc_lrm}        f1 Score : {f1_lrm}")
print(f"\n Random Forest Classifer : Accuracy : {acc_rfc}        f1 Score : {f1_rfc}")
print(f"\n SVM : Accuracy : {acc_svm}        f1 Score : {f1_svm}")

print(f"\n Confusion Matrix for Logistic Regression : \n {cm_lrm}")

print(f"\n Confusion Matrix for Random Forest Classifer : \n {cm_rfc}")

print(f"\n Confusion Matrix for SVM : \n {cm_svm}")